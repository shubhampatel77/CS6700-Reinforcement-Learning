{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Actor-Critic & DQN algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras\n",
    "from typing import Any, List, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining Model and Agent classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Actor Critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticModel(keras.Model):\n",
    "    \"\"\"\n",
    "    Defining policy and value networkss\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, n_hidden1=1024, n_hidden2=512):\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "\n",
    "        #Hidden Layer 1\n",
    "        self.fc1 = layers.Dense(n_hidden1, activation='relu')\n",
    "        #Hidden Layer 2\n",
    "        self.fc2 = layers.Dense(n_hidden2, activation='relu')\n",
    "\n",
    "        self.fc3 = layers.Dense(128, activation='relu')\n",
    "        \n",
    "        #Output Layer for policy\n",
    "        self.pi_out = layers.Dense(action_size, activation='softmax')\n",
    "        #Output Layer for state-value\n",
    "        self.v_out = layers.Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        \"\"\"\n",
    "        Computes policy distribution and state-value for a given state\n",
    "        \"\"\"\n",
    "        layer1 = self.fc1(state)\n",
    "        layer2 = self.fc2(layer1)\n",
    "        layer3 = self.fc3(layer2)\n",
    "\n",
    "        pi = self.pi_out(layer3)\n",
    "        v = self.v_out(layer3)\n",
    "\n",
    "        return pi, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, lr=0.001, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.action = None\n",
    "        self.ac_model = ActorCriticModel(action_size=action_size)\n",
    "        self.ac_model.compile(tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "        # np.random.seed(seed)\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, compute the policy distribution over all actions and sample one action\n",
    "        \"\"\"\n",
    "        state = tf.convert_to_tensor([state])\n",
    "        pi, _ = self.ac_model(state)\n",
    "\n",
    "        action_probabilities = tfp.distributions.Categorical(probs=pi)\n",
    "        sample = action_probabilities.sample()\n",
    "        self.action = sample\n",
    "\n",
    "        return sample.numpy()[0]\n",
    "\n",
    "    # def actor_loss(self, action, pi, delta):\n",
    "    #     \"\"\"\n",
    "    #     Compute Actor Loss\n",
    "    #     \"\"\"\n",
    "    #     return -tf.math.log(pi[0, action]) * delta\n",
    "\n",
    "    # def critic_loss(self,delta):\n",
    "    #     \"\"\"\n",
    "    #     Critic loss aims to minimize TD error\n",
    "    #     \"\"\"\n",
    "    #     return delta**2\n",
    "\n",
    "    # @tf.function\n",
    "    def learn(self, state, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        For a given transition (s,a,s',r) update the paramters by computing the\n",
    "        gradient of the total loss\n",
    "        \"\"\"\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        next_state = tf.convert_to_tensor([next_state], dtype=tf.float32)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            pi, V_s = self.ac_model(state)\n",
    "            _, V_s_next = self.ac_model(next_state)\n",
    "\n",
    "            # if done:\n",
    "            #     V_s_next = tf.stop_gradient(V_s_next)\n",
    "\n",
    "            V_s = tf.squeeze(V_s)\n",
    "            V_s_next = tf.squeeze(V_s_next)\n",
    "\n",
    "            action_probs = tfp.distributions.Categorical(probs=pi)\n",
    "            log_prob = action_probs.log_prob(self.action)\n",
    "\n",
    "            delta = reward + (self.gamma * V_s_next*(1-int(done))) - V_s\n",
    "\n",
    "            loss_a = -log_prob*delta#self.actor_loss(action, pi, delta)\n",
    "            loss_c = delta**2\n",
    "            loss_total = loss_a + loss_c\n",
    "\n",
    "        gradient = tape.gradient(loss_total, self.ac_model.trainable_variables)\n",
    "        self.ac_model.optimizer.apply_gradients(zip(gradient, self.ac_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Actor-Critic algoritm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"### Train the Network\"\"\"\n",
    "\n",
    "avg_list = []\n",
    "\n",
    "ENVIRONMENTS = ['Acrobot-v1', 'CartPole-v1', 'MountainCar-v0']\n",
    "\n",
    "# for envs in ENVIRONMENTS:\n",
    "env = gym.make('CartPole-v1')\n",
    "#Initializing Agent\n",
    "agent = Agent(lr=1e-5, action_size=env.action_space.n)\n",
    "#Number of episodes\n",
    "episodes = 1500\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "\n",
    "filename = 'cartpole_1e-5_1024x512_1800games.png'\n",
    "\n",
    "figure_file = 'plots/' + filename\n",
    "\n",
    "reward_list = []\n",
    "average_reward_list = []\n",
    "start = time.time()\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.sample_action(state) ##Sample Action\n",
    "        next_state, reward, done, info, _ = env.step(action) ##Take action\n",
    "        episode_reward += reward  ##Updating episode reward\n",
    "        agent.learn(state, reward, next_state, done) ##Update Parameters\n",
    "        state = next_state ##Updating State\n",
    "    reward_list.append(episode_reward)\n",
    "    average_reward_list.append(np.mean(reward_list[-100:]))\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        avg_reward = np.mean(reward_list[-10:])\n",
    "        print('Episode ', episode, 'Reward %f' % episode_reward, 'Average Reward %f' % avg_reward)\n",
    "\n",
    "    if episode % 100:\n",
    "        avg_100 =  np.mean(reward_list[-100:])\n",
    "        if avg_100 > 495.0:\n",
    "            print('Stopped at Episode ', episode-100)\n",
    "            break\n",
    "\n",
    "end = time.time()   \n",
    "print(f\"Total training time for AC algorithm: {end-start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Plotting Reward curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting total reward curve (moving average over 100 episodes)\n",
    "\"\"\"\n",
    "episodes=1260\n",
    "\n",
    "avg_var = []\n",
    "for i in range(episodes):\n",
    "    avg_var.append(np.var(reward_list[i:100+i]))\n",
    "\n",
    "#Plot of total reward vs episode\n",
    "print(f\"Maximum Reward obtained = {np.max(average_reward_list)} at episode {np.argmax(average_reward_list)+1})\")\n",
    "plt.plot(np.arange(episodes), average_reward_list)\n",
    "plt.title(\"Running Avg of 100 rewards\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()\n",
    "\n",
    "#Plot of Reward Variance vs episode\n",
    "plt.plot(np.arange(episodes), avg_var)\n",
    "plt.title(\"Reward Variance Plot\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Variance Of Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Rendering an episode and saving as a GIF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render an episode and save as a GIF file\n",
    "\n",
    "# display = Display(visible=0, size=(400, 300))\n",
    "# display.start()\n",
    "from PIL import Image\n",
    "\n",
    "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
    "  screen = env.render(mode='rgb_array')\n",
    "  print(screen)\n",
    "  im = Image.fromarray(screen)\n",
    "\n",
    "  images = [im]\n",
    "  \n",
    "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "  for i in range(1, max_steps + 1):\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    action_probs, _ = model(state)\n",
    "    action = np.argmax(np.squeeze(action_probs))\n",
    "    state, _, done, _ = env.step(action)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "\n",
    "    # Render screen every 10 steps\n",
    "    if i % 10 == 0:\n",
    "      screen = env.render(mode='rgb_array')\n",
    "      images.append(Image.fromarray(screen))\n",
    "  \n",
    "    if done:\n",
    "      break\n",
    "  \n",
    "  return images\n",
    "\n",
    "\n",
    "# Save GIF image\n",
    "images = render_episode(env, agent.ac_model, 200)\n",
    "image_file = 'cartpole-v1.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "\n",
    "# import tensorflow_docs.vis.embed as embed\n",
    "# embed.embed_file(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Full Step Returns\"\"\"\n",
    "\n",
    "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
    "# This would allow it to be included in a callable TensorFlow graph.\n",
    "\n",
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  return (state.astype(np.float32), \n",
    "          np.array(reward, np.int32), \n",
    "          np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "  return tf.numpy_function(env_step, [action], \n",
    "                           [tf.float32, tf.int32, tf.int32])\n",
    "\n",
    "eps = 1e-2\n",
    "Avg_List = []\n",
    "\n",
    "def run_episode(\n",
    "    initial_state: tf.Tensor,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "\n",
    "  for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "  \n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_logits_t, value = model(state)\n",
    "  \n",
    "    # Sample next action from the action probability distribution\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "    # Store critic values\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "  \n",
    "    # Apply action to the environment to get next state and reward\n",
    "    state, reward, done = tf_env_step(action)\n",
    "    state.set_shape(initial_state_shape)\n",
    "  \n",
    "    # Store reward\n",
    "    rewards = rewards.write(t, reward)\n",
    "\n",
    "    if tf.cast(done, tf.bool):\n",
    "      break\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "  \n",
    "  return action_probs, values, rewards\n",
    "\n",
    "class FullReturnsAgentTF:\n",
    "  def __init__(self, action_size,n_hidden1=1024,n_hidden2=512,n_hidden3=None, lr=0.001, gamma=0.99, seed = 85):\n",
    "        self.gamma = gamma\n",
    "        self.ac_model = ActorCriticModel(action_size=action_size,n_hidden1=n_hidden1,n_hidden2=n_hidden2)\n",
    "        self.ac_model.compile(tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "        np.random.seed(seed)\n",
    "  #         ##calculating expected returns##\n",
    "  def get_expected_return(self,\n",
    "      rewards: tf.Tensor, \n",
    "      gamma: float, \n",
    "      standardize: bool = True) -> tf.Tensor:\n",
    "    \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    # Start from the end of `rewards` and accumulate reward sums\n",
    "    # into the `returns` array\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "      reward = rewards[i]\n",
    "      discounted_sum = reward + gamma * discounted_sum\n",
    "      discounted_sum.set_shape(discounted_sum_shape)\n",
    "      returns = returns.write(i, discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "\n",
    "    if standardize:\n",
    "      returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "                (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "    return returns\n",
    "  def get_expected_return_n_step(self, rewards: tf.Tensor, values: tf.Tensor, gamma: float, no_of_steps: int, standardize: bool = True) -> tf.Tensor:\n",
    "      \"\"\"Compute n-step returns per timestep.\"\"\"\n",
    "\n",
    "      a = gamma**(no_of_steps)\n",
    "      b = gamma**(no_of_steps - 1)\n",
    "      n = tf.shape(rewards)[0]\n",
    "      returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "      # Start from the end of `rewards` and accumulate reward sums\n",
    "      # into the `returns` array\n",
    "      rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "      discounted_sum = tf.constant(0.0)\n",
    "      discounted_sum_shape = discounted_sum.shape\n",
    "      if no_of_steps == 1000:\n",
    "        no_of_steps = n\n",
    "      for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        if i + no_of_steps >= n:\n",
    "          discounted_sum = reward + gamma * discounted_sum\n",
    "        else:\n",
    "          discounted_sum = reward + gamma * discounted_sum + a*values[i+no_of_steps]\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i, discounted_sum)\n",
    "        if i + no_of_steps < n:\n",
    "          discounted_sum -= a*values[i+no_of_steps]\n",
    "        if i + no_of_steps < n + 1:\n",
    "          discounted_sum -= b*rewards[i+no_of_steps-1]\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "      returns = returns.stack()[::-1]\n",
    "\n",
    "      if standardize:\n",
    "        returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "                  (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "      return returns\n",
    "  #             ##calculating AC loss##\n",
    "  huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "  \n",
    "\n",
    "  def compute_loss(self,\n",
    "      action_probs: tf.Tensor,  \n",
    "      values: tf.Tensor,  \n",
    "      returns: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "    \n",
    "    advantage = returns - values\n",
    "\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "    # tf.cast(values, tf.float32); tf.cast(returns, tf.float32)\n",
    "    N = tf.cast(tf.shape(values)[0],tf.float32)\n",
    "    critic_loss = N*self.mse(values, returns)\n",
    "\n",
    "    return actor_loss + critic_loss\n",
    "\n",
    "  #         ## Train func for updating parameters##\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self,\n",
    "      initial_state: tf.Tensor, \n",
    "      model: tf.keras.Model, \n",
    "      optimizer: tf.keras.optimizers.Optimizer, \n",
    "      gamma: float, \n",
    "      max_steps_per_episode: int) -> tf.Tensor:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      # Run the model for one episode to collect training data\n",
    "      action_probs, values, rewards = run_episode(\n",
    "          initial_state, model, max_steps_per_episode) \n",
    "\n",
    "      # Calculate expected returns\n",
    "      returns = self.get_expected_return_n_step(rewards, values, gamma, 7)\n",
    "\n",
    "      # Convert training data to appropriate TF tensor shapes\n",
    "      action_probs, values, returns = [\n",
    "          tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "\n",
    "      # Calculating loss values to update our network\n",
    "      loss = self.compute_loss(action_probs, values, returns)\n",
    "\n",
    "    # Compute the gradients from the loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the model's parameters\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "    return episode_reward\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %time\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#Initializing Agent\n",
    "agent = FullReturnsAgentTF(lr=1e-4, action_size=env.action_space.n)\n",
    "#Number of episodes\n",
    "episodes = 500\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "reward_list = []\n",
    "average_reward_list = []\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "avg_rew_prev=0\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 5000 # their given value = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "# Cartpole-v1 is considered solved if average reward is >= 495 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 495\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "  for i in t:\n",
    "    # agent = FullReturnsAgentTF()\n",
    "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "    episode_reward = int(agent.train_step(\n",
    "        initial_state, agent.ac_model, agent.ac_model.optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "    episodes_reward.append(episode_reward)\n",
    "    running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "    reward_list.append(episode_reward)\n",
    "    average_reward_list.append(np.mean(reward_list[-100:]))\n",
    "\n",
    "    \n",
    "\n",
    "    t.set_description(f'Episode {i}')\n",
    "    t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "    # # Show average episode reward every 10 episodes\n",
    "    # if i % 10 == 0:\n",
    "    #   pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "    # if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "    #     break\n",
    "    # reward_list.append(ep_rew)\n",
    "    # average_reward_list.append(np.mean(reward_list[-100:]))\n",
    "\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        avg_rew = np.mean(reward_list[-10:])\n",
    "        print('Episode ', i, 'Reward %f' % episode_reward, 'Average Reward %f' % avg_rew)\n",
    "        if (avg_rew > avg_rew_prev and avg_rew > 100):\n",
    "          model_dir_name = 'my_model'+str(i//10)\n",
    "          # agent.ac_model.save(model_file_name)\n",
    "          tf.saved_model.save(agent.ac_model,model_dir_name)\n",
    "        avg_rew_prev = avg_rew\n",
    "\n",
    "    if i % 100:\n",
    "        avg_100 =  np.mean(reward_list[-100:])\n",
    "        if avg_100 > 475.0:\n",
    "            print('Stopped at Episode ',i-100)\n",
    "            break\n",
    "Avg_List.append(average_reward_list)\n",
    "# print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\n",
    "time_taken = datetime.datetime.now() - begin_time\n",
    "print(time_taken)\n",
    "\n",
    "Avg_List.append(average_reward_list)\n",
    "print(len(Avg_List))\n",
    "\n",
    "mx = -1\n",
    "for i in Avg_List:\n",
    "  print(len(i))\n",
    "  mx = max(mx, len(i))\n",
    "print(mx)\n",
    "\n",
    "for i in range(len(Avg_List)):\n",
    "  Avg_List[i] += [Avg_List[i][-1]] * (mx - len(Avg_List[i]))\n",
    "\n",
    "Avg_val = np.mean(Avg_List, axis = 0)\n",
    "Avg_var = np.var(Avg_List, axis = 0)\n",
    "\n",
    "print(len(Avg_val), len(Avg_var))\n",
    "\n",
    "### Plot of total reward vs episode\n",
    "## Write Code Below\n",
    "plt.plot(np.arange(len(average_reward_list)), average_reward_list)\n",
    "plt.title(\"Running Avg of 100 rewards\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()\n",
    "\n",
    "### Plot of Reward Variance vs episode\n",
    "## Write Code Below\n",
    "plt.plot(np.arange(len(Avg_var)), Avg_var)\n",
    "plt.title(\"Reward Variance Plot\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Variance Of Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "import tensorflow as tf\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "'''\n",
    "Please refer to the first tutorial for more details on the specifics of environments\n",
    "We've only added important commands you might find useful for experiments.\n",
    "'''\n",
    "\n",
    "'''\n",
    "List of example environments\n",
    "(Source - https://gym.openai.com/envs/#classic_control)\n",
    "\n",
    "'Acrobot-v1'\n",
    "'Cartpole-v1'\n",
    "'MountainCar-v0'\n",
    "'''\n",
    "\n",
    "#%%\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "seed = 0\n",
    "env.reset(seed=seed)\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "\n",
    "print(state_shape)\n",
    "print(no_of_actions)\n",
    "print(env.action_space.sample())\n",
    "print(\"----\")\n",
    "\n",
    "'''\n",
    "# Understanding State, Action, Reward Dynamics\n",
    "\n",
    "The agent decides an action to take depending on the state.\n",
    "\n",
    "The Environment keeps a variable specifically for the current state.\n",
    "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
    "- It returns the new current state and reward for the agent to take the next action\n",
    "\n",
    "'''\n",
    "\n",
    "state = env.reset(seed=seed)[0].reshape(1,-1)   \n",
    "''' This returns the initial state (when environment is reset) '''\n",
    "\n",
    "print(state)\n",
    "print(\"----\")\n",
    "\n",
    "action = env.action_space.sample()  \n",
    "''' We take a random action now '''\n",
    "\n",
    "print(action)\n",
    "print(\"----\")\n",
    "\n",
    "next_state, reward, done, other, _ = env.step(action) \n",
    "''' env.step is used to calculate new state and obtain reward based on old state and action taken  ''' \n",
    "\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(other)\n",
    "print(\"----\")\n",
    "\n",
    "\"\"\"## DQN\n",
    "\n",
    "Using NNs as substitutes isn't something new. It has been tried earlier, but the 'human control' paper really popularised using NNs by providing a few stability ideas (Q-Targets, Experience Replay & Truncation). The 'Deep-Q Network' (DQN) Algorithm can be broken down into having the following components. \n",
    "\n",
    "### Q-Network:\n",
    "The neural network used as a function approximator is defined below\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "### Q Network & Some 'hyperparameters'\n",
    "\n",
    "QNetwork1:\n",
    "Input Layer - 4 nodes (State Shape) \\\n",
    "Hidden Layer 1 - 64 nodes \\\n",
    "Hidden Layer 2 - 64 nodes \\\n",
    "Output Layer - 2 nodes (Action Space) \\\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "QNetwork2: Feel free to experiment more\n",
    "'''\n",
    "#%%\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "'''\n",
    "Bunch of Hyper parameters (Which you might have to tune later **wink wink**)\n",
    "'''\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size [int(1e6), same]\n",
    "BATCH_SIZE = 64         # minibatch size [512, 128]\n",
    "GAMMA = 0.99            # discount factor [same, 0.999] [0.5, 0.7, 0.99]\n",
    "LR = 5e-4               # learning rate [same, 1e-4]\n",
    "UPDATE_EVERY = 20       # how often to update the network (When Q target is present) [same, 30], [10, 20, 40]\n",
    "\n",
    "\n",
    "class QNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork1, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\"\"\"### Replay Buffer:\n",
    "\n",
    "This is a 'deque' that helps us store experiences. Recall why we use such a technique.\n",
    "\"\"\"\n",
    "\n",
    "#%%\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "\"\"\"## Truncation:\n",
    "We add a line (optionally) in the code to truncate the gradient in hopes that it would help with the stability of the learning process.\n",
    "\n",
    "## Tutorial Agent Code:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#%%\n",
    "class TutorialAgent():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units, fc2_units):\n",
    "\n",
    "        ''' Agent Environment Interaction '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        ''' Q-Network '''\n",
    "        self.qnetwork_local = QNetwork1(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
    "        self.qnetwork_target = QNetwork1(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        ''' Replay memory '''\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        ''' Save experience in replay memory '''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        ''' If enough samples are available in memory, get random subset and learn '''\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
    "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''      \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        ''' Epsilon-greedy action selection (Already Present) '''\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ''' Get max predicted Q values (for next states) from target model'''\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ''' Compute Q targets for current states '''\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        ''' Get expected Q values from local model '''\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ''' Compute loss '''\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        ''' Minimize the loss '''\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        ''' Gradiant Clipping '''\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        self.optimizer.step()\n",
    "\n",
    "\"\"\"### Here, we present the DQN algorithm code.\"\"\"\n",
    "\n",
    "''' Defining DQN Algorithm '''\n",
    "#%%\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "def dqn(n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "\n",
    "    scores = []                 \n",
    "    ''' list containing scores from each episode '''\n",
    "\n",
    "    steps = []\n",
    "    steps_window = []\n",
    "\n",
    "    scores_window_printing = deque(maxlen=10) \n",
    "    ''' For printing in the graph '''\n",
    "    \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
    "\n",
    "    eps = eps_start                    \n",
    "    ''' initialize epsilon '''\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset(seed=seed)[0].reshape(1,-1) \n",
    "        score = 0\n",
    "        step_episode = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, other, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                step_episode = t\n",
    "                break \n",
    "\n",
    "        scores_window.append(score)       \n",
    "        scores_window_printing.append(score)   \n",
    "        ''' save most recent score '''  \n",
    "        steps_window.append(step_episode)         \n",
    "\n",
    "        eps = max(eps_end, eps_decay*eps) \n",
    "        ''' decrease epsilon '''\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
    "        if i_episode % 10 == 0: \n",
    "            scores.append(np.mean(scores_window_printing))\n",
    "            steps.append(np.mean(step_episode))        \n",
    "        if i_episode % 100 == 0: \n",
    "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "    return np.array(scores), np.array(steps), i_episode-100\n",
    "\n",
    "#%%\n",
    "total_runs = 10\n",
    "n_episodes = 1500\n",
    "GAMMA = 0.99     \n",
    "\n",
    "combinations = [\n",
    "    [5e-4, int(1e5), 20, 64, [128, 64]],\n",
    "    [1e-3, int(1e5), 20, 64, [128, 64]],\n",
    "    [5e-4, int(1e5), 25, 128, [128, 64]],\n",
    "    [5e-4, int(1e3), 50, 64, [128, 64]],\n",
    "    [5e-4, int(1e7), 20, 32, [64, 64]],\n",
    "    [1e-4, int(1e5), 5, 64, [128, 64]],\n",
    "    [5e-4, int(1e5), 25, 128, [256, 128]],\n",
    "    [1e-4, int(1e5), 20, 128, [128, 128]],\n",
    "    [1e-3, int(1e5), 20, 256, [256, 128]],\n",
    "    [5e-4, int(1e3), 10, 256, [256, 128]],\n",
    "    [1e-4, int(1e5), 50, 256, [256, 128]],\n",
    "    [5e-4, int(1e5), 20, 256, [256, 256]],\n",
    "]\n",
    "\n",
    "parameter_names = [\"LR\", \"BUFFER_SIZE\", \"UPDATE_EVERY\", \"BATCH_SIZE\", \"ARCHITECTURE\"]\n",
    "\n",
    "combinationsDict = {}\n",
    "for i_name in range(len(parameter_names)):\n",
    "    parameter_list = []\n",
    "    for j_combo in range(len(combinations)):\n",
    "        parameter_list.append(combinations[j_combo][i_name])\n",
    "        combinationsDict.setdefault(parameter_names[i_name], parameter_list)\n",
    "\n",
    "combo_stats = []\n",
    "for i in range(len(combinations)):\n",
    "    BUFFER_SIZE = combinationsDict[\"BUFFER_SIZE\"][i]\n",
    "    BATCH_SIZE = combinationsDict[\"BATCH_SIZE\"][i]\n",
    "    UPDATE_EVERY = combinationsDict[\"UPDATE_EVERY\"][i]\n",
    "    LR = combinationsDict[\"LR\"][i]\n",
    "    ARCHITECTURE = combinationsDict[\"ARCHITECTURE\"][i]\n",
    "\n",
    "    begin_time = datetime.datetime.now()\n",
    "    agent = TutorialAgent(state_size=state_shape, action_size=action_shape, \n",
    "                          seed=0, fc1_units=ARCHITECTURE[0], fc2_units=ARCHITECTURE[1])\n",
    "    run_stats = []\n",
    "    for run in range(total_runs):\n",
    "        run_stats.append(dqn(n_episodes=n_episodes))\n",
    "    combo_stats.append(run_stats)\n",
    "    \n",
    "    time_taken = datetime.datetime.now() - begin_time\n",
    "    print(\"Combination \" + str(i+1) + \" - \" + str(time_taken))\n",
    "\n",
    "#%%\n",
    "run_stats_matrix = np.zeros((len(combo_stats),total_runs,len(combo_stats[0][0][0])))\n",
    "for i in range(len(combo_stats)):\n",
    "    for j in range(total_runs):\n",
    "        for k in range(len(combo_stats[0][0][0])):\n",
    "            run_stats_matrix[i][j][k] = combo_stats[i][j][0][k] \n",
    "\n",
    "#%%\n",
    "for i in range(len(combo_stats)):\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(10,n_episodes+1,10), np.mean(run_stats_matrix[i], axis=0))\n",
    "    plt.savefig(\"DQNcomb_\"+str(i)+\".png\")\n",
    "    plt.show()\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
