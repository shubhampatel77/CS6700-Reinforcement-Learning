{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Heirarchical RL: SMDP & Intra Option algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym.wrappers import Monitor \n",
    "import glob\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, clear_output \n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\") # env.seed(0)\n",
    "state_shape = 1 #Discrete(500) no_of_actions = env.action_space.n\n",
    "print(\"State shape\",state_shape)\n",
    "print(\"Number of Actions\", no_of_actions)\n",
    "print(env.action_space.sample())\n",
    "print(\"----\")\n",
    "    # Actions\n",
    "    # There are 6 discrete deterministic actions:\n",
    "    # - 0: move south\n",
    "    # - 1: move north\n",
    "    # - 2: move east\n",
    "    # - 3: move west\n",
    "    # - 4: pickup passenger\n",
    "    # - 5: drop off passenger\n",
    "state = env.reset()\n",
    "''' This returns the initial state (when environment is reset) '''\n",
    "print(state)\n",
    "print(\"----\")\n",
    "action = env.action_space.sample() \n",
    "''' We take a random action now '''\n",
    "print(action)\n",
    "print(\"----\")\n",
    "next_state, reward, done, info = env.step(action)\n",
    "''' env.step is used to calculate new state and obtain reward based on old state and ac tion taken '''\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done, info = env.step(0)\n",
    "env.render()\n",
    "list(env.decode(next_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_red = [[1,3,0,0,0],\n",
    "             [1,3,0,0,0],\n",
    "             [1,3,3,3,3],\n",
    "             [1,1,1,1,1],\n",
    "             [1,1,1,1,1]]\n",
    "route_green = [[0,0,2,2,1],\n",
    "             [0,0,2,2,1],\n",
    "             [2,2,2,2,1],\n",
    "             [1,1,1,1,1],\n",
    "             [1,1,1,1,1]]\n",
    "route_yellow = [[0,0,0,0,0],\n",
    "             [0,0,0,0,0],\n",
    "             [0,3,3,3,3],\n",
    "             [0,1,1,1,1],\n",
    "             [0,1,1,1,1]]\n",
    "route_blue = [[0,0,0,0,0],\n",
    "             [0,0,0,0,0],\n",
    "             [2,2,2,0,3],\n",
    "             [1,1,1,0,3],\n",
    "             [1,1,1,0,3]]\n",
    "loc = {0: 'Red',\n",
    "       1: 'Green',\n",
    "       2: 'Yellow',\n",
    "       3: 'Blue',\n",
    "       4: 'Taxi'}\n",
    "\n",
    "action_name = {0: 'S',\n",
    "                1: 'N',\n",
    "                2: 'E',\n",
    "                3: 'W',\n",
    "                4: 'P',\n",
    "                5: 'D',\n",
    "                6: 'R',\n",
    "                7: 'G',\n",
    "                8: 'Y',\n",
    "                9: 'B'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_map = {6: route_red,\n",
    "             7: route_green,\n",
    "             8: route_yellow,\n",
    "             9: route_blue}\n",
    "def Option(env, state, loc):\n",
    "    taxi_pos = list(env.decode(next_state))\n",
    "    optact = route_map[loc][taxi_pos[0]][taxi_pos[1]] optdone = False\n",
    "    if loc==6 and taxi_pos[0]==0 and taxi_pos[1]==0: optdone = True if loc==7 and taxi_pos[0]==0 and taxi_pos[1]==4: optdone = True if loc==8 and taxi_pos[0]==4 and taxi_pos[1]==0: optdone = True if loc==9 and taxi_pos[0]==4 and taxi_pos[1]==3: optdone = True\n",
    "return [optact, optdone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Table: (States x Actions) === (env.ns(500) x total actions(10))\n",
    "q_values = np.zeros((500,10))\n",
    "# TODO: epsilon-greedy action selection function\n",
    "def egreedy_policy(q_values,state,epsilon):\n",
    "    rg = np.random.RandomState()\n",
    "    if not q_values[state].any() or rg.rand() < epsilon:\n",
    "        return rg.choice(q_values.shape[-1])\n",
    "    else:\n",
    "        return np.argmax(q_values[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SMDP Q-Learning\n",
    "q_values = np.zeros((500,10))\n",
    "updates = np.zeros((500,10))\n",
    "rewards = []\n",
    "# Add parameters you might need here\n",
    "gamma = 0.9\n",
    "alpha = 0.4\n",
    "# Iterate over 1000 episodes\n",
    "for epi in tqdm(range(2000)): \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    tot_reward = 0\n",
    "    # While episode is not over\n",
    "    while not done:\n",
    "        # Choose action\n",
    "        action = egreedy_policy(q_values, state, epsilon=0.1) # print(state, action)\n",
    "        # Checking if primitive action\n",
    "        if action < 6:\n",
    "        # Perform regular Q-Learning update for state-action pair\n",
    "            next_state, reward, done,_ = env.step(action)\n",
    "            q_values[state, action] = q_values[state, action] + alpha*(reward + gamma*np.max(q_values[next_state, :]) - q_values[state, action])\n",
    "            updates[state, action] += 1\n",
    "            state = next_state\n",
    "            tot_reward += reward\n",
    "        # Checking if action chosen is an option\n",
    "        reward_bar = 0 \n",
    "        if action >= 6:\n",
    "            optdone = False\n",
    "            cnt = 0\n",
    "            prev = state\n",
    "            while (optdone == False):\n",
    "                # Think about what this function might do?\n",
    "                optact,optdone = Option(env,state,action)\n",
    "                next_state, reward, done,_ = env.step(optact)\n",
    "                # Is this formulation right? What is this term?\n",
    "                reward_bar = reward_bar + reward*(gamma**cnt)\n",
    "                cnt += 1\n",
    "                tot_reward += reward\n",
    "                # Complete SMDP Q-Learning Update\n",
    "                # Remember SMDP Updates. When & What do you update?\n",
    "                state = next_state \n",
    "                if(done): break\n",
    "            q_values[prev, action] += alpha*(reward_bar + (gamma**cnt)*np.max(q_values[state, :]) - q_values[prev, action])\n",
    "            updates[prev, action] += 1\n",
    "    rewards.append(tot_reward)\n",
    "q_values_SMDP = q_values.copy()\n",
    "updates_SMDP = updates.copy()\n",
    "rewards_SMDP = rewards.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(rewards, name='SMDP'): \n",
    "    plt.title(f'Reward Curve for {name}')\n",
    "    plt.xlabel('Episodes') plt.ylabel('Reward')\n",
    "    plt.plot(rewards) plt.savefig(f'{name}.png')\n",
    "    plt.show()\n",
    "    print(\"Average Reward for last 500 episodes\", np.mean(rewards[-500:]))\n",
    "\n",
    "\n",
    "plot_stats(rewards_SMDP, 'SMDP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qmatrix(q_values, i, j): \n",
    "    qmatrix = np.zeros((5,5,10)) \n",
    "    for row in range(5):\n",
    "        for col in range(5):\n",
    "            qmatrix[row][col][:] = np.copy(q_values[env.encode(row,col,i,j)])\n",
    "    return qmatrix\n",
    "\n",
    "def vizQ(q_values): \n",
    "    for i in range(5):\n",
    "        for j in range(4):\n",
    "            if i == j: continue\n",
    "            print(f\"Passenger at {loc[i]} going to {loc[j]}\") \n",
    "            for row in range(5):\n",
    "                for col in range(5): \n",
    "                    print(action_name[np.argmax(q_values[env.encode(row,col,i,j)])],end=\" \")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 1\n",
    "DOWN = 0\n",
    "LEFT = 3\n",
    "RIGHT = 2\n",
    "def plot_Q(q_values):\n",
    "    fig, axs = plt.subplots(5, 4, figsize=(50,50))\n",
    "    for i in range(5):\n",
    "        for j in range(4):\n",
    "            #if i==j: continue\n",
    "            Q = Qmatrix(q_values_SMDP,i,j)\n",
    "            #fig, ax = plt.subplots()\n",
    "            # plt.figure(figsize=(5,5))\n",
    "            axs[i, j].set_title(f\"Passenger at {loc[i]} going to {loc[j]}\") \n",
    "            axs[i, j].invert_yaxis()\n",
    "\n",
    "            axs[i, j].pcolor(Q.max(-1), edgecolors='k', linewidths=2) \n",
    "            # plt.colorbar()\n",
    "            def x_direct(a):\n",
    "            if a == RIGHT: return 1 if a == LEFT: return -1 return 0\n",
    "            \n",
    "            def y_direct(a):\n",
    "            if a == UP: return 1 if a == DOWN: return -1 return 0\n",
    "            policy = Q.argmax(-1)\n",
    "            policyx = np.vectorize(x_direct)(policy)\n",
    "            policyy = np.vectorize(y_direct)(policy)\n",
    "            idx = np.indices(policy.shape)\n",
    "            axs[i, j].quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
    "        \n",
    "            for k in range(25):\n",
    "                act = policy.reshape(25,)[k]\n",
    "                if act < 4: continue\n",
    "                axs[i, j].annotate(f'{action_name[act]}', xy=(idx[1].ravel()[k]+0.5,idx[0].ravel()[k]+0.5), fontsize=20, ha='center', va='center')\n",
    "        \n",
    "    plt.show()\n",
    "pltQ = plot_Q(q_values_SMDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Intra-option Q-Learning\n",
    "q_values = np.zeros((500,10))\n",
    "updates = np.zeros((500,10))\n",
    "rewards = []\n",
    "# Add parameters you might need here\n",
    "gamma = 0.9\n",
    "alpha = 0.5\n",
    "# Iterate over 1000 episodes\n",
    "for _ in tqdm(range(2000)):\n",
    "    state = env.reset() \n",
    "    done = False \n",
    "    tot_reward = 0\n",
    "    # While episode is not over\n",
    "    while not done:\n",
    "        # Choose action\n",
    "        action = egreedy_policy(q_values, state, epsilon=0.1) # print(state, action)\n",
    "        # Checking if primitive action\n",
    "        if action < 6:\n",
    "            # Perform regular Q-Learning update for state-action pair\n",
    "            next_state, reward, done,_ = env.step(action)\n",
    "            q_values[state, action] = q_values[state, action] + alpha*(reward + gamma*np.max(q_values[next_state, :]) - q_values[state, action])\n",
    "            updates[state, action] += 1\n",
    "            state = next_state\n",
    "            tot_reward += reward\n",
    "            # Checking if action chosen is an option \n",
    "        if action >= 6: # action => Top option\n",
    "            optdone = False\n",
    "            cnt = 0\n",
    "            prev = state\n",
    "            while (optdone == False):\n",
    "                # Think about what this function might do?\n",
    "                optact,optdone = Option(env,state,action)\n",
    "                next_state, reward, done,_ = env.step(optact)\n",
    "                tot_reward += reward\n",
    "                # Complete SMDP Q-Learning Update\n",
    "                # Remember SMDP Updates. When & What do you update?\n",
    "                optact_tmp,optdone_tmp = Option(env,next_state,action)\n",
    "                q_values[state, optact] += alpha*(reward + gamma*np.max(q_values[next_state, :]) - q_values[state, optact])\n",
    "                updates[state, optact] += 1 \n",
    "                if(optdone_tmp):\n",
    "                    q_values[state, action] += alpha*(reward + gamma*np.max(q_values[next_state, :]) - q_values[state, action])\n",
    "                    updates[state, action] += 1\n",
    "                    state = next_state\n",
    "                    break \n",
    "                else:\n",
    "                    q_values[state, action] += alpha*(reward + gamma*q_values[next_state, action] - q_values[state, action])\n",
    "                    updates[state, action] += 1 \n",
    "                state = next_state\n",
    "                if(done): \n",
    "                    break\n",
    "    rewards.append(tot_reward)\n",
    "q_values_intra_option = q_values.copy()\n",
    "updates_intra_option = updates.copy()\n",
    "rewards_intra_option = rewards.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(rewards_intra_option, 'Intra Option')\n",
    "pltQ = plot_Q(q_values_intra_option)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
