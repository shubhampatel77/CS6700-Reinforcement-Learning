{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 1: SARSA & Q-Learning Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjUHahLwtejy"
      },
      "outputs": [],
      "source": [
        "from math import floor\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "import gridworld\n",
        "from gridworld import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsWb5W9NGgCw"
      },
      "source": [
        "## 1. Environment Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH-IvmRxBirA"
      },
      "outputs": [],
      "source": [
        "UP = 0\n",
        "DOWN = 1\n",
        "LEFT = 2\n",
        "RIGHT = 3\n",
        "actions = [UP, DOWN, LEFT, RIGHT]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8SE5M-ODuOF"
      },
      "outputs": [],
      "source": [
        "#Heatmap function\n",
        "def plot_Q(Q, message = \"Q plot\"):\n",
        "    \n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.title(message)\n",
        "    plt.pcolor(Q.max(-1), edgecolors='k', linewidths=2)\n",
        "    plt.colorbar()\n",
        "    def x_direct(a):\n",
        "        if a in [UP, DOWN]:\n",
        "            return 0\n",
        "        return 1 if a == RIGHT else -1\n",
        "    def y_direct(a):\n",
        "        if a in [RIGHT, LEFT]:\n",
        "            return 0\n",
        "        return 1 if a == UP else -1\n",
        "    policy = Q.argmax(-1)\n",
        "    policyx = np.vectorize(x_direct)(policy)\n",
        "    policyy = np.vectorize(y_direct)(policy)\n",
        "    idx = np.indices(policy.shape)\n",
        "    plt.quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsZrGEyZCcYZ"
      },
      "outputs": [],
      "source": [
        "#World parameters\n",
        "num_rows = 10\n",
        "num_cols = 10\n",
        "obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
        "                         [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
        "                         [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
        "                         [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
        "bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
        "restart_states = np.array([[3,7],[8,2]])\n",
        "start_state = np.array([[0,4]])\n",
        "goal_states = np.array([[0,9],[2,2],[8,7]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Visualising the grid\n",
        "def heatmap(num_rows, num_cols, obstructions, bad_states, restart_states, start_state, goal_states):\n",
        "    \n",
        "    grid = np.zeros([num_rows, num_cols])\n",
        "    state_list = [obstructions, bad_states, restart_states, start_state, goal_states]\n",
        "    for i in range(len(state_list)):\n",
        "        for r in range(state_list[i].shape[0]):\n",
        "            grid[state_list[i][r][0], state_list[i][r][1]] = heat[i]\n",
        "    return grid\n",
        "\n",
        "heat = [10, 6, 8, 5, 3]\n",
        "grid = heatmap(num_rows, num_cols, obstructions, bad_states, restart_states, start_state, goal_states)\n",
        "sns.set()\n",
        "ax = sns.heatmap(grid, vmin=0, vmax=10, cmap=\"Blues\")\n",
        "sns.color_palette(\"tab10\")\n",
        "plt.show()\n",
        "print(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Creating Grid World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uNDbk_9ChYL"
      },
      "outputs": [],
      "source": [
        "#Creating model\n",
        "gw = gridworld.GridWorld(num_rows=num_rows,\n",
        "               num_cols=num_cols,\n",
        "               start_state=start_state,\n",
        "               goal_states=goal_states, wind = False)\n",
        "gw.add_obstructions(obstructed_states=obstructions,\n",
        "                    bad_states=bad_states,\n",
        "                    restart_states=restart_states)\n",
        "gw.add_rewards(step_reward=-1,\n",
        "               goal_reward=10,\n",
        "               bad_state_reward=-6,\n",
        "               restart_state_reward=-100)\n",
        "\n",
        "gw.add_transition_probability(p_good_transition=1.0, bias=0.5)\n",
        "\n",
        "env = gw.create_gridworld()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVuyIb5cCva8",
        "outputId": "326106d4-7f49-4f02-97d6-262999276e85"
      },
      "outputs": [],
      "source": [
        "print(\"Number of actions\", env.num_actions) #0 -> UP, 1-> DOWN, 2 -> LEFT, 3-> RIGHT\n",
        "print(\"Number of states\", env.num_states)\n",
        "print(\"start state\", env.start_state_seq)\n",
        "print(\"goal state(s)\", env.goal_states_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAMqj52XEomd"
      },
      "source": [
        "## 3. Learning Algorithms: Q Learning & SARSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02em5zORhbY4"
      },
      "source": [
        "### Exploration strategies\n",
        "1. Epsilon-greedy\n",
        "2. Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-07T18:54:35.904993Z",
          "start_time": "2019-11-07T18:54:35.897975Z"
        },
        "id": "1ZusJ9LqhbY6"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "alpha0 = 0.4\n",
        "gamma0 = 0.9\n",
        "episodes = 10000\n",
        "test_episodes = 1000\n",
        "epsilon0 = 0.1\n",
        "beta0 = 0.1 #[0.001, 1.0, 5.0, 50.0]\n",
        "max_steps = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-07T18:54:35.895680Z",
          "start_time": "2019-11-07T18:54:35.859394Z"
        },
        "id": "UixpqLxNhbY6"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "seed = 42\n",
        "rg = np.random.RandomState(seed)\n",
        "\n",
        "#Epsilon greedy\n",
        "def choose_action_epsilon(Q, state, epsilon, rg=rg):\n",
        "    if rg.rand() < epsilon:\n",
        "        return rg.randint(len(actions))\n",
        "    else:\n",
        "        return np.argmax(Q[state[0,0]][state[0,1]])\n",
        "\n",
        "#Softmax\n",
        "def choose_action_softmax(Q, state, beta, rg=rg):\n",
        "    prob = softmax(Q[state[0,0]][state[0,1]]/beta)\n",
        "    best_action = rg.choice(actions, p = prob)\n",
        "    return best_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkzBJ_pjhbY6"
      },
      "source": [
        "### 3.1 SARSA Algorithm\n",
        "Update equation:\n",
        "\\begin{equation}\n",
        "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EUo5L64hbY6"
      },
      "source": [
        "#### Hyperparameters\n",
        "\n",
        "We have 3 hyperparameters for the algorithm:\n",
        "- $\\alpha$\n",
        "- number of *episodes*.\n",
        "- $\\epsilon$: For epsilon greedy exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-07T18:54:35.917916Z",
          "start_time": "2019-11-07T18:54:35.907076Z"
        },
        "id": "JdSmq79fhbY7"
      },
      "outputs": [],
      "source": [
        "print_freq = 100\n",
        "\n",
        "def sarsa(env, Q, gamma, alpha, policy_parameter, episodes, plot_heat, choose_action):\n",
        "\n",
        "    episode_rewards = np.zeros(episodes)\n",
        "    steps_to_completion = np.zeros(episodes)\n",
        "    if plot_heat:\n",
        "        clear_output(wait=True)\n",
        "        plot_Q(Q)\n",
        "\n",
        "    state_visits = np.zeros((num_rows, num_cols))\n",
        "    for ep in range(episodes):\n",
        "        tot_reward, steps = 0, 0\n",
        "        \n",
        "        #Reset environment\n",
        "        state = env.reset()\n",
        "        action = choose_action(Q, seq_to_col_row(state, num_cols), policy_parameter)\n",
        "        # done = False\n",
        "        while steps<max_steps:\n",
        "            state_next, reward = env.step(state,action)\n",
        "            state_rc = seq_to_col_row(state, num_cols)\n",
        "            state_next_rc = seq_to_col_row(state_next, num_cols)\n",
        "            state_visits[state_rc[:,0], state_rc[:,1]] += 1\n",
        "\n",
        "            #print(state_next,gw.goal_states_seq)\n",
        "            if any(np.sum(np.abs(gw.goal_states-state_rc),1)==0):\n",
        "                break\n",
        "            action_next = choose_action(Q, seq_to_col_row(state_next, num_cols), policy_parameter)\n",
        "            \n",
        "            Q[state_rc[:,0], state_rc[:,1], actions.index(action)] = Q[state_rc[:,0], state_rc[:,1], actions.index(action)] + alpha*(reward + gamma*Q[state_next_rc[:,0], state_next_rc[:,1], actions.index(action_next)] - Q[state_rc[:,0], state_rc[:,1], actions.index(action)])\n",
        "                                                    \n",
        "            tot_reward += reward\n",
        "            steps += 1\n",
        "            \n",
        "            state, action = state_next, action_next\n",
        "        \n",
        "        episode_rewards[ep] = tot_reward\n",
        "        steps_to_completion[ep] = steps\n",
        "\n",
        "        if (ep+1)%print_freq == 0 and plot_heat:\n",
        "            clear_output(wait=True)\n",
        "            plot_Q(Q, message = f\"alpha={alpha},gamma={gamma},epsilon={policy_parameter} \"+\"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(ep+1, np.mean(episode_rewards[ep-print_freq+1:ep]),\n",
        "                                                                           np.mean(steps_to_completion[ep-print_freq+1:ep]),\n",
        "                                                                           Q.max(), Q.min()))\n",
        "                \n",
        "    return Q, episode_rewards, steps_to_completion, state_visits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Q-Learning Algorithm\n",
        "Update rule for Q-Learning:\n",
        "\\begin{equation}\n",
        "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_freq = 100\n",
        "\n",
        "def qlearning(env, Q, gamma, alpha, policy_parameter, plot_heat = False, choose_action = choose_action_softmax):\n",
        "\n",
        "    episode_rewards = np.zeros(episodes)\n",
        "    steps_to_completion = np.zeros(episodes)\n",
        "    \n",
        "    if plot_heat:\n",
        "        clear_output(wait=True)\n",
        "        plot_Q(Q)\n",
        "    \n",
        "    for ep in range(episodes):\n",
        "        tot_reward, steps = 0, 0\n",
        "        \n",
        "        #Reset environment\n",
        "\n",
        "        state = env.reset()\n",
        "        action = choose_action(Q, seq_to_col_row(state, num_cols), policy_parameter)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            state_next, reward, done = env.step(action)\n",
        "            action_next = choose_action(Q, state_next)\n",
        "            \n",
        "            Q[state[0], state[1], actions.index(action)] = Q[state[0], state[1], actions.index(action)] + alpha*(reward + gamma*np.max(Q[state_next[0], state_next[1], :]) - Q[state[0], state[1], actions.index(action)])\n",
        "                                                    \n",
        "            tot_reward += reward\n",
        "            steps += 1\n",
        "            \n",
        "            state, action = state_next, action_next\n",
        "        \n",
        "        episode_rewards[ep] = tot_reward\n",
        "        steps_to_completion[ep] = steps\n",
        "        \n",
        "        if (ep+1)%print_freq == 0 and plot_heat:\n",
        "            clear_output(wait=True)\n",
        "            plot_Q(Q, message = \"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(ep+1, np.mean(episode_rewards[ep-print_freq+1:ep]),\n",
        "                                                                           np.mean(steps_to_completion[ep-print_freq+1:ep]),\n",
        "                                                                           Q.max(), Q.min()))\n",
        "                \n",
        "    return Q, episode_rewards, steps_to_completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Hyperparameter Search\n",
        "Performing search for optimum set hyperparameters for each of the 16 configurations for each algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Initializing test hyperparameters \n",
        "test_steps = []\n",
        "test_rewards = []\n",
        "\n",
        "#Choosing values both emperically and after some test runs\n",
        "test_alpha_list = [0.001, 0.4, 0.9]\n",
        "test_beta_list = [0.1, 1.0, 5.0]\n",
        "test_epsilon_list = [0.0001, 0.2]\n",
        "test_gamma_list = [0, 0.4, .9]\n",
        "\n",
        "test_parameters = [test_alpha_list, test_beta_list, test_epsilon_list, test_gamma_list]\n",
        "parameter_names = ['Alpha', 'Beta', 'Epsilon', 'Gamma']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimum_combination_steps(test_array, test_policy_parameter):\n",
        "    '''\n",
        "    Returns\n",
        "    '''\n",
        "    indices = np.where(test_array == np.min(test_array))\n",
        "    return test_alpha_list[indices[0][0]], test_policy_parameter[indices[1][0]], test_gamma_list[indices[2][0]]\n",
        "\n",
        "def hyperparam_search(algorithm, plot_heat):\n",
        "    \n",
        "    #Initializing matrices for optimum params\n",
        "    optimum_alpha_ep = np.zeros((2,2,2))\n",
        "    optimum_alpha_soft = np.zeros((2,2,2))\n",
        "\n",
        "    optimum_epsilon_ep = np.zeros((2,2,2))\n",
        "\n",
        "    optimum_beta_soft = np.zeros((2,2,2))\n",
        "\n",
        "    optimum_gamma_ep = np.zeros((2,2,2))\n",
        "    optimum_gamma_soft = np.zeros((2,2,2))\n",
        "\n",
        "    configuration = 1\n",
        "\n",
        "    #Looping over all configurations\n",
        "    for wind in [False, True]:\n",
        "        for start_state in [np.array([[0,4]]), np.array([[3,6]])]:\n",
        "            for p_good_transition in [1.0, 0.7]:\n",
        "            \n",
        "                gw = gridworld.GridWorld(num_rows=num_rows, num_cols=num_cols, start_state=start_state, goal_states=goal_states, wind=wind)\n",
        "                gw.add_obstructions(obstructed_states=obstructions, bad_states=bad_states, restart_states=restart_states)\n",
        "                gw.add_rewards(step_reward=-1, goal_reward=10, bad_state_reward=-6, restart_state_reward=-100)\n",
        "                gw.add_transition_probability(p_good_transition=p_good_transition, bias=0.5)\n",
        "                env = gw.create_gridworld()\n",
        "                \n",
        "                if wind == False:\n",
        "                    i = 0\n",
        "                else: \n",
        "                    i = 1\n",
        "\n",
        "                if (start_state == np.array([[0,4]])).all():\n",
        "                    j = 0\n",
        "                else:\n",
        "                    j = 1\n",
        "                \n",
        "                if p_good_transition == 1.0:\n",
        "                    k = 0\n",
        "                else:\n",
        "                    k = 1\n",
        "\n",
        "                test_steps_ep = np.zeros((3, 2, 3))\n",
        "                test_rewards_ep = np.zeros((3, 2, 3))\n",
        "\n",
        "                print(f\"Searching for configuration: {configuration}\", \"\\r\")\n",
        "                \n",
        "                start = time.time()\n",
        "                for alpha in test_parameters[0]:\n",
        "                    for epsilon in test_parameters[2]:\n",
        "                        for gamma in test_parameters[3]:\n",
        "                            Q = np.zeros((env.num_rows, env.num_cols, len(actions)))    \n",
        "                            Q, rewards, steps, state_visits = algorithm(env, Q, gamma, alpha, epsilon, test_episodes, plot_heat, choose_action = choose_action_epsilon)\n",
        "                            test_steps_ep[test_parameters[0].index(alpha), test_parameters[2].index(epsilon), test_parameters[3].index(gamma)] = np.mean(steps)\n",
        "                            test_rewards_ep[test_parameters[0].index(alpha), test_parameters[2].index(epsilon), test_parameters[3].index(gamma)] = np.mean(rewards)\n",
        "\n",
        "                test_steps_soft = np.zeros((3, 3, 3))\n",
        "                test_rewards_soft = np.zeros((3, 3, 3))\n",
        "                \n",
        "                for alpha in test_parameters[0]:\n",
        "                    for beta in test_parameters[1]:\n",
        "                        for gamma in test_parameters[3]:\n",
        "                            Q = np.zeros((env.num_rows, env.num_cols, len(actions)))    \n",
        "                            Q, rewards, steps, state_visits = algorithm(env, Q, gamma, alpha, beta, test_episodes, plot_heat, choose_action = choose_action_softmax)\n",
        "                            test_steps_soft[test_parameters[0].index(alpha), test_parameters[1].index(beta), test_parameters[3].index(gamma)] = np.mean(steps)\n",
        "                            test_rewards_soft[test_parameters[0].index(alpha), test_parameters[1].index(beta), test_parameters[3].index(gamma)] = np.mean(rewards)\n",
        "\n",
        "                end = time.time()\n",
        "\n",
        "                a, e, g = optimum_combination_steps(test_steps_ep, test_epsilon_list)\n",
        "                optimum_alpha_ep[i, j, k] = a\n",
        "                optimum_epsilon_ep[i, j, k] = e\n",
        "                optimum_gamma_ep[i, j, k] = g\n",
        "                # optimum_combination_rewards(test_rewards_soft, test_epsilon_list)\n",
        "                a, b, g = optimum_combination_steps(test_steps_soft, test_beta_list)\n",
        "                # optimum_combination_rewards(test_rewards_soft, test_beta_list)\n",
        "                optimum_alpha_soft[i, j, k] = a\n",
        "                optimum_beta_soft[i, j, k] = b\n",
        "                optimum_gamma_soft[i, j, k] = g\n",
        "\n",
        "                print(f\"Time of execution for configuration {configuration} = {end-start}s\")\n",
        "                configuration += 1\n",
        "\n",
        "    return [[optimum_alpha_ep, optimum_epsilon_ep, optimum_gamma_ep], [optimum_alpha_soft, optimum_beta_soft, optimum_gamma_soft]]  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Control the *plot_heat* function as you like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimum_sarsa = hyperparam_search(sarsa,plot_heat=False)\n",
        "optimum_qlearning = hyperparam_search(qlearning, plot_heat=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(optimum_alpha_soft)\n",
        "# print('\\n---------\\n')\n",
        "# print(optimum_beta_soft)\n",
        "# print('\\n---------\\n')\n",
        "# print(optimum_gamma_soft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN7RDuMsnujy"
      },
      "outputs": [],
      "source": [
        "# num_expts = 5\n",
        "# reward_avgs, steps_avgs = [], []\n",
        "\n",
        "# for i in range(num_expts):\n",
        "#     print(\"Experiment: %d\"%(i+1))\n",
        "#     Q = np.zeros((env.num_rows, env.num_cols, len(actions)))\n",
        "#     rg = np.random.RandomState(i)\n",
        "\n",
        "#     Q, rewards, steps = sarsa(env, Q, gamma = gamma, plot_heat=True, choose_action= choose_action_softmax)\n",
        "#     steps_avgs.append(steps)\n",
        "#     reward_avgs.append(rewards)\n",
        "\n",
        "# steps_avgs = np.sum(steps_avgs,axis=0)/num_expts\n",
        "# reward_avgs = np.sum(reward_avgs,axis=0)/num_expts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypq7lSiPqSST"
      },
      "outputs": [],
      "source": [
        "# plt.figure()\n",
        "# plt.plot(range(1000),steps_avgs)\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Number of steps to Goal')\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(range(1000),reward_avgs)\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Total Reward')\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zZ-oE2BEEhgy",
        "hsWb5W9NGgCw",
        "02em5zORhbY4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
